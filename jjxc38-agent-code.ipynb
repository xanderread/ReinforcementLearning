{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j45ZlafdiB1X"
      },
      "source": [
        "# Dancing like the Inbetweeners: Learning to Walk with Symphony\n",
        "\n",
        "Code Notebook By Alex Read - jjxc38\n",
        "\n",
        "All references in report - main referenced codebases:\n",
        "\n",
        "\n",
        "[Symphony original implementation\n",
        "](https://github.com/timurgepard/Simphony)\n",
        "\n",
        "[TD3 Implementaton\n",
        "](https://github.com/lukau2357/bipedal-walker-td3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTNU1mwGB1ZD"
      },
      "source": [
        "**Dependencies and setup**\n",
        "\n",
        "This can take a minute or so..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rA38jtUgtZsG"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install setuptools==65.5.0 \"wheel<0.40.0\"\n",
        "!apt update\n",
        "!apt-get install python3-opengl\n",
        "!apt install xvfb -y\n",
        "!pip install 'swig'\n",
        "!pip install 'pyglet==1.5.27'\n",
        "!pip install 'gym[box2d]==0.20.0'\n",
        "!pip install 'pyvirtualdisplay==3.0'\n",
        "\n",
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as disp\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jRlQRjFiJ3t"
      },
      "source": [
        "**Additional Setup**\n",
        "- we use weights & biases for logging\n",
        "- we use optuna for hyperparameter optimisation\n",
        "- set seeds/settings early on, to make sure everything is deterministic!\n",
        "- **select environment normal / hardcore**\n",
        "- start logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "-4hcosNLnVtc",
        "outputId": "17275d75-4943-4223-9d33-1bf94ca24578"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.13.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.8.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.29)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.3)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.11.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mflipperft\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.6"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240505_201850-e7mwywkt</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/flipperft/BipedalWalker-v3/runs/e7mwywkt' target=\"_blank\">star-federation-434</a></strong> to <a href='https://wandb.ai/flipperft/BipedalWalker-v3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/flipperft/BipedalWalker-v3' target=\"_blank\">https://wandb.ai/flipperft/BipedalWalker-v3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/flipperft/BipedalWalker-v3/runs/e7mwywkt' target=\"_blank\">https://wandb.ai/flipperft/BipedalWalker-v3/runs/e7mwywkt</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/flipperft/BipedalWalker-v3/runs/e7mwywkt?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x789edbb3f4c0>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install wandb -qU # import wandb for logging\n",
        "!pip install optuna\n",
        "\n",
        "import wandb\n",
        "import copy\n",
        "import math\n",
        "import time\n",
        "\n",
        "#setup wandb\n",
        "api_key = 'rickrolled'\n",
        "wandb.login(key=api_key)\n",
        "\n",
        "\n",
        "display = Display(visible=0,size=(600,600))\n",
        "display.start()\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "\n",
        "plot_interval = 10 # update the plot every N episodes\n",
        "video_every = 100 # videos can take a very long time to render so only do it every N episodes\n",
        "\n",
        "\n",
        "\n",
        "# Set seeds nice and early - for consistent results\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.use_deterministic_algorithms(True)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "# Setup logging / Choose enviroment\n",
        "%load_ext wandb\n",
        "ENV_NAME = \"BipedalWalker-v3\" #change to \"BipedalWalkerHardcore-v3\" for the hardcore enviroment\n",
        "wandb.init(project=ENV_NAME,save_code=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USFVtP6LkOcR"
      },
      "source": [
        "**Reinforcement learning agent**\n",
        "\n",
        "We reimplement the actor-critic agent nicknamed '[Symphony](https://github.com/timurgepard/Simphony/tree/main/2_1)' from the book ['How to make Robot move like a Human with Reinforcement Learning'](https://https://www.amazon.co.uk/gp/product/B0CKYWHPF5/ref=ppx_yo_dt_b_d_asin_title_351_o06?ie=UTF8&psc=1).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xd-pKs3En1xP"
      },
      "source": [
        "Loss Functions\n",
        "\n",
        "The Rectified Huber Error (ReHE)\n",
        "and Rectified Huber Asymmetrical Error (ReHAE)\n",
        "$$\n",
        "\\text{ReHAE}(x) = \\text{abs}(x) \\cdot \\tanh(x)\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "Huber loss distinguishes between small and large errors and combines properties of mean squared error for small errors and mean absolute error for large errors. This Rectified Huber Loss with tanh does a similar thing only with the tanh function. In our experiments, we tried a variety of loss functions.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84HfW1mOvxpU"
      },
      "outputs": [],
      "source": [
        "# modified huber loss functions\n",
        "def ReHE(error):\n",
        "    ae = torch.abs(error).mean()\n",
        "    return ae*torch.tanh(ae)\n",
        "\n",
        "def ReHaE(error):\n",
        "    e = error.mean()\n",
        "    return torch.abs(e)*torch.tanh(e)\n",
        "\n",
        "# standard huber loss functions\n",
        "def hE(error, delta):\n",
        "    abs_error = torch.abs(error)\n",
        "    quadratic_part = torch.clamp(abs_error - delta, min=0.0)\n",
        "    linear_part = delta * (abs_error - 0.5 * delta)\n",
        "    return torch.mean(0.5 * quadratic_part**2 + linear_part)\n",
        "\n",
        "def hAE(error, delta):\n",
        "    abs_error = torch.abs(error)\n",
        "    return torch.mean(torch.where(abs_error <= delta, 0.5 * error**2, delta * (abs_error - 0.5 * delta)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3JNwgF2aimQ"
      },
      "source": [
        "Network\n",
        "\n",
        "The main network architecture uses sine functions, just like a Fourier series! The idea is that neural networks structured as a Fourier series [have been effective in modelling periodic patterns and tasks with cyclical behaviour](https://onlinelibrary.wiley.com/doi/full/10.1002/sam.11531). This network works with the normal bipedal walker great; however, the Fourier series architecture struggles in the hardcore environment which has non-periodic and irregular patterns.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5xbKAE5bZ3_"
      },
      "outputs": [],
      "source": [
        "class ReSine(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return F.leaky_relu(torch.sin(x), 0.1)\n",
        "\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self, hidden_dim, f_out):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fft = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            ReSine(),\n",
        "            nn.Linear(hidden_dim, f_out)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fft(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QecDZAY1nKbs"
      },
      "source": [
        "Actor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ymqj7MOmaTE"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, obs_dim, act_dim, device, hidden_dim=256, max_action=1.0,exploration_scale_factor=0.2,noise_limit_multiplier = 2.5):\n",
        "        super(Actor, self).__init__()\n",
        "        self.device = device\n",
        "\n",
        "        # transform input observations to match the size of the neural network's input layer\n",
        "        self.input = nn.Linear(obs_dim, hidden_dim)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            Network(hidden_dim, act_dim),\n",
        "            nn.Tanh() # squashes the output to the range (-1,1)\n",
        "        )\n",
        "\n",
        "        # calculate hyperparameters for noise / exploration\n",
        "        self.exploration_scale_factor = exploration_scale_factor\n",
        "        self.max_action = torch.mean(max_action).item()\n",
        "        self.noise_scale = self.exploration_scale_factor*self.max_action # calculate scale for adding noise / exploration\n",
        "        self.noise_limit = noise_limit_multiplier*self.noise_scale # limit for magnitude of noise\n",
        "\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = self.input(state)\n",
        "        x = self.max_action*self.net(x)\n",
        "        return x\n",
        "\n",
        "    def action(self, state):\n",
        "        self.input.eval()\n",
        "        with torch.no_grad():\n",
        "            x = self.forward(state)\n",
        "            x += (self.exploration_scale_factor*torch.randn_like(x)).clamp(-self.noise_limit, self.noise_limit)\n",
        "        self.input.train()\n",
        "        return x.clamp(-self.max_action, self.max_action)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbdm3WVFoori"
      },
      "source": [
        "Critic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_EFo-JJ4xrx"
      },
      "outputs": [],
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, obs_dim, act_dim, hidden_dim=256):\n",
        "        super(Critic, self).__init__()\n",
        "        self.input = nn.Linear(obs_dim+act_dim, hidden_dim)\n",
        "        # Q-value function approximations\n",
        "        qA = Network(hidden_dim, 1)\n",
        "        qB = Network(hidden_dim, 1)\n",
        "        qC = Network(hidden_dim, 1)\n",
        "\n",
        "        self.nets = nn.ModuleList([qA, qB, qC])\n",
        "\n",
        "\n",
        "    def forward(self, state, action, united=False):\n",
        "        x = torch.cat([state, action], -1)\n",
        "        x = self.input(x)\n",
        "        xs = [net(x) for net in self.nets]\n",
        "\n",
        "        # united returns a unified Q-value by taking the minimum across the networks\n",
        "        if not united: return xs\n",
        "\n",
        "        # otherwise return a list containing the Q-values calculated by each network\n",
        "        return torch.min(torch.cat(xs, dim=-1), dim=-1, keepdim=True).values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1T7ot8frqKyy"
      },
      "source": [
        "Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CT3d_B994mV_"
      },
      "outputs": [],
      "source": [
        "# Define the actor-critic agent\n",
        "class Agent(object):\n",
        "    def __init__(self, obs_dim, act_dim, hidden_dim, device, max_action=1.0, fade_factor=7.0, alpha=0.07,exploration_scale_factor=0.2):\n",
        "\n",
        "        # agent has a replay buffer\n",
        "        self.replay_buffer = ReplayBuffer(obs_dim, act_dim, device, fade_factor, alpha)\n",
        "\n",
        "        # setup actor\n",
        "        self.actor = Actor(obs_dim, act_dim, device, hidden_dim, max_action=max_action,exploration_scale_factor=exploration_scale_factor).to(device)\n",
        "\n",
        "        # use three critics, two online and one offline critic\n",
        "        self.critic = Critic(obs_dim, act_dim, hidden_dim).to(device)\n",
        "        self.critic2 = copy.deepcopy(self.critic)\n",
        "\n",
        "        self.critic_target = copy.deepcopy(self.critic)\n",
        "\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=3e-4)\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=3e-4)\n",
        "        self.critic2_optimizer = optim.Adam(self.critic2.parameters(), lr=3e-4)\n",
        "\n",
        "        self.max_action = max_action\n",
        "        self.device = device\n",
        "        self.obs_dim = obs_dim\n",
        "        self.act_dim = act_dim\n",
        "        self.q_old_policy = 0.0\n",
        "\n",
        "\n",
        "\n",
        "    def sample_action(self, states):\n",
        "        with torch.no_grad():\n",
        "            states = np.array(states)\n",
        "            state = torch.FloatTensor(states).reshape(-1,self.obs_dim).to(self.device)\n",
        "            action = self.actor.action(state)\n",
        "        return action.cpu().data.numpy().flatten()\n",
        "\n",
        "\n",
        "    def train(self, tr_per_step=1):\n",
        "        #Update-To-Data (UTD)\n",
        "        for _ in range(tr_per_step):\n",
        "            state, action, reward, next_state, done = self.replay_buffer.sample()\n",
        "            #Sample Multiple Reuse (SMR) #TODO: need to look into this and really just cleanup this function in general\n",
        "            for _ in range(3):\n",
        "                self.critic_update(state, action, reward, next_state, done)\n",
        "            self.actor_update(state)\n",
        "    # update critic\n",
        "    def critic_update(self, state, action, reward, next_state, done):\n",
        "        with torch.no_grad():\n",
        "            for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
        "                target_param.data.copy_(0.997*target_param.data + 0.003*param)\n",
        "\n",
        "            next_action = self.actor(next_state)\n",
        "            q_next_target = self.critic_target(next_state, next_action, united=True)\n",
        "            q_value = reward +  (1-done) * 0.99 * q_next_target\n",
        "        qs = self.critic(state, action, united=False)\n",
        "        other_qs = self.critic2(state, action, united=False)\n",
        "        other_critic_loss = ReHE(q_value - qs[0]) + ReHE(q_value - qs[1]) + ReHE(q_value - qs[2])\n",
        "        critic_loss = ReHE(q_value - qs[0]) + ReHE(q_value - qs[1]) + ReHE(q_value - qs[2])\n",
        "        critic_loss = min(other_critic_loss, critic_loss)\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "    def actor_update(self, state):\n",
        "        action = self.actor(state)\n",
        "        q_new_policy = self.critic(state, action, united=True)\n",
        "        actor_loss = -ReHaE(q_new_policy - self.q_old_policy)\n",
        "\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward(retain_graph=True)\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        with torch.no_grad(): self.q_old_policy = q_new_policy.mean().detach()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DirDytBoqPZJ"
      },
      "source": [
        "Replay Buffer\n",
        "- We use a faded replay - older experiences are valued less"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9W3WmlFM48Gr"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, obs_dim, act_dim, device, fade_factor=7.0, alpha=0.07,capacity=512000,delta_max=3.0,delta_max_update_threshold=1000):\n",
        "        self.length = 0 # current length of buffer\n",
        "        self.device = device\n",
        "        self.capacity = capacity # represents the maximum number of experiences (state, action, reward, next state, done) that the buffer can store\n",
        "        self.batch_size = min(max(128, self.length//500), 1024) #   determines how many experiences are sampled from the buffer for each training iteration. It's dynamically set based on the length of the buffer\n",
        "        self.fade_factor = fade_factor # controls how much older experiences are weighted during sampling\n",
        "        self.alpha_base = alpha # scales the importance of recent rewards in adjusting the alpha value, which is used in reward correction.\n",
        "        self.delta_max = delta_max #represents the largest observed difference between consecutive states\n",
        "        self.delta_max_update_threshold = delta_max_update_threshold # represents a threshold (timesteps) for updating delta_max\n",
        "        self.alpha = alpha # dynamically adjusted based on the average difference between consecutive states. This adjustment regulates how much the reward correction term affects the overall reward.\n",
        "\n",
        "        # init vars\n",
        "        self.indices, self.indexes, self.probs, self.step = [], np.array([]), np.array([]), 0\n",
        "        self.states = torch.zeros((self.capacity, obs_dim), dtype=torch.float32).to(device)\n",
        "        self.actions = torch.zeros((self.capacity, act_dim), dtype=torch.float32).to(device)\n",
        "        self.rewards = torch.zeros((self.capacity, 1), dtype=torch.float32).to(device)\n",
        "        self.next_states = torch.zeros((self.capacity, obs_dim), dtype=torch.float32).to(device)\n",
        "        self.dones = torch.zeros((self.capacity, 1), dtype=torch.float32).to(device)\n",
        "        self.rewards_sum = 0\n",
        "\n",
        "\n",
        "    # add a memory to the buffer\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "\n",
        "      # if the buffer is not at max size increased its size\n",
        "        if self.length<self.capacity:\n",
        "            self.length += 1\n",
        "            self.indices.append(self.length-1)\n",
        "            self.indexes = np.array(self.indices)\n",
        "\n",
        "        idx = self.length-1\n",
        "\n",
        "        # update running sum of rewards and total step count\n",
        "        self.rewards_sum += (reward/2)*math.tanh(reward/2)\n",
        "        self.step += 1\n",
        "\n",
        "\n",
        "        # update alpha value based on rewards and steps\n",
        "        self.alpha = self.alpha_base*(10.0 + self.rewards_sum/self.step)/10.0\n",
        "\n",
        "        # compute delta (mean absolute difference between states)\n",
        "        delta = np.mean(np.abs(next_state - state)).item()\n",
        "\n",
        "\n",
        "        # update delta_max if step count is less than or equal to delta_max_update_threshold\n",
        "        if self.step<=self.delta_max_update_threshold:\n",
        "            if delta>self.delta_max: self.delta_max = delta\n",
        "\n",
        "        # normalise and adjust reward using alpha and delta\n",
        "        delta /= self.delta_max\n",
        "        reward += self.alpha*(0.5*math.tanh(math.log(2.0*delta))+0.5)\n",
        "\n",
        "        # store the experience in the buffer\n",
        "        self.states[idx,:] = torch.FloatTensor(state).to(self.device)\n",
        "        self.actions[idx,:] = torch.FloatTensor(action).to(self.device)\n",
        "        self.rewards[idx,:] = torch.FloatTensor([reward]).to(self.device)\n",
        "        self.next_states[idx,:] = torch.FloatTensor(next_state).to(self.device)\n",
        "        self.dones[idx,:] = torch.FloatTensor([done]).to(self.device)\n",
        "\n",
        "        # update batch size based on current buffer length\n",
        "        self.batch_size = min(max(128, self.length//250), 2048)\n",
        "\n",
        "        # if buffer is full, roll the buffer to remove the oldest experience\n",
        "        if self.length==self.capacity:\n",
        "            self.states = torch.roll(self.states, shifts=-1, dims=0)\n",
        "            self.actions = torch.roll(self.actions, shifts=-1, dims=0)\n",
        "            self.rewards = torch.roll(self.rewards, shifts=-1, dims=0)\n",
        "            self.next_states = torch.roll(self.next_states, shifts=-1, dims=0)\n",
        "            self.dones = torch.roll(self.dones, shifts=-1, dims=0)\n",
        "\n",
        "\n",
        "    # produces probabilities of sampling something from the replay buffer - if something is recent it should have a higher probability of being chosen\n",
        "    def generate_probs(self):\n",
        "        if self.step>self.capacity: return self.probs\n",
        "        def fade(norm_index): return np.tanh(self.fade_factor*norm_index**2) # linear / -> non-linear _/‾\n",
        "        weights = 1e-7*(fade(self.indexes/self.length))# weights are based solely on the history, highly squashed\n",
        "        self.probs = weights/np.sum(weights)\n",
        "        return self.probs\n",
        "\n",
        "    # sample from replay buffer\n",
        "    def sample(self):\n",
        "        indices = np.random.choice(self.indexes, p=self.generate_probs(), size=self.batch_size)\n",
        "        return (\n",
        "            self.states[indices],\n",
        "            self.actions[indices],\n",
        "            self.rewards[indices],\n",
        "            self.next_states[indices],\n",
        "            self.dones[indices]\n",
        "        )\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEv4ZjXmyrHo"
      },
      "source": [
        "**Prepare the environment and wrap it to capture videos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Xrcek4hxDXl"
      },
      "outputs": [],
      "source": [
        "\n",
        "%%capture\n",
        "env = gym.make(ENV_NAME) # env_name is defined at the top of the notebook\n",
        "# env = gym.make(\"BipedalWalkerHardcore-v3\") # only attempt this when your agent has solved BipedalWalker-v3\n",
        "env = gym.wrappers.Monitor(env, \"./video\", video_callable=lambda ep_id: ep_id%video_every == 0, force=True)\n",
        "\n",
        "obs_dim = env.observation_space.shape[0]\n",
        "act_dim = env.action_space.shape[0]\n",
        "\n",
        "\n",
        "env.seed(seed)\n",
        "env.action_space.seed(seed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvwPy1MWsHeC",
        "outputId": "0f1abf48-ebf9-42e8-8aaf-34b3ab534140"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The environment has 24 observations and the agent can take 4 actions\n",
            "The device is: cpu\n"
          ]
        }
      ],
      "source": [
        "print('The environment has {} observations and the agent can take {} actions'.format(obs_dim, act_dim))\n",
        "print('The device is: {}'.format(device))\n",
        "\n",
        "if device.type != 'cpu': print('It\\'s recommended to train on the cpu for this')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQqOGQissJJA"
      },
      "outputs": [],
      "source": [
        "# used to log data to weights and biases\n",
        "def log_to_wandb(episode, total_reward_avg, total_reward_std, episode_reward):\n",
        "  wandb.log({\n",
        "        'episode': episode,\n",
        "        'total_reward_avg': total_reward_avg,\n",
        "        'episode_reward': episode_reward\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbFFUXMbsKv5",
        "outputId": "3cc1d95b-f468-46bc-e109-870e6cba0873"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hyperparamaters being used:  {'explore_time': 500, 'tr_per_step': 5, 'hidden_dim': 258, 'fade_factor': 7, 'alpha': 0.02000001342, 'exploration_scale_factor': 0.2, 'tr_pre_step': 64}\n"
          ]
        }
      ],
      "source": [
        "explore_time = 500 # time where we don't train, just fill replay buffer\n",
        "tr_per_step = 5 # actor's updates per frame/step\n",
        "tr_pre_step = 64 # actor's updates before a frame/step\n",
        "hidden_dim = 258 # dimensions of network to use\n",
        "fade_factor = 7 # fading memory factor\n",
        "alpha = 0.02000001342 # basicially a stall penalty\n",
        "exploration_scale_factor=0.2 # exploration scale factor\n",
        "\n",
        "hyperparams = {\n",
        "    'explore_time': explore_time,\n",
        "    'tr_per_step': tr_per_step,\n",
        "    'hidden_dim': hidden_dim,\n",
        "    'fade_factor': fade_factor,\n",
        "    'alpha':alpha,\n",
        "    'exploration_scale_factor':exploration_scale_factor,\n",
        "    'tr_pre_step':tr_pre_step\n",
        "}\n",
        "\n",
        "print(\"Hyperparamaters being used: \", hyperparams)\n",
        "wandb.config.update(hyperparams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "G25KgJO1vsb-",
        "outputId": "edc89bc5-5d48-43af-cece-b0d45989dd81"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# needed variables\n",
        "Q_learning = False\n",
        "max_action = 1.0\n",
        "max_action = max_action*torch.FloatTensor(env.action_space.high).to(device) if env.action_space.is_bounded() else max_action*1.0\n",
        "total_steps = 0\n",
        "\n",
        "\n",
        "# is used to init weights of the actor - this is still seeded / deterministic!\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear): torch.nn.init.xavier_uniform_(m.weight)\n",
        "\n",
        "# chris's logging variables\n",
        "ep_reward = 0\n",
        "reward_list = []\n",
        "plot_data = []\n",
        "log_f = open(\"agent-log.txt\",\"w+\")\n",
        "\n",
        "# initialise agent\n",
        "agent = Agent(obs_dim, act_dim, hidden_dim, device, max_action, fade_factor, alpha,exploration_scale_factor)\n",
        "max_episodes = 1000\n",
        "max_timesteps = 2000\n",
        "\n",
        "\n",
        "# training procedure:\n",
        "for episode in range(1, max_episodes+1):\n",
        "    state = env.reset()\n",
        "\n",
        "\n",
        "    # If the agent is exploring, sample some random weights for it - these are seeded!\n",
        "    if not Q_learning and total_steps < explore_time:\n",
        "      agent.actor.apply(init_weights)\n",
        "\n",
        "\n",
        "    # for some reason got errors without, seems to relieve cpu / gpu\n",
        "    if Q_learning: time.sleep(0.5)\n",
        "\n",
        "\n",
        "    for t in range(max_timesteps):\n",
        "        total_steps += 1\n",
        "\n",
        "\n",
        "        if t>=explore_time and not Q_learning:\n",
        "            print(\"end of exploration\")\n",
        "            Q_learning = True\n",
        "\n",
        "            agent.train(tr_pre_step)\n",
        "\n",
        "        action = agent.sample_action(state)\n",
        "\n",
        "\n",
        "        # take action in environment and get r and s'\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "\n",
        "\n",
        "        # add to replay buffer\n",
        "        agent.replay_buffer.add(state, action, reward, next_state, done)\n",
        "\n",
        "\n",
        "        if Q_learning:\n",
        "          agent.train(tr_per_step)\n",
        "\n",
        "        state = next_state\n",
        "        ep_reward += reward\n",
        "\n",
        "        #\n",
        "        if done or (not Q_learning and t>=2000): break\n",
        "\n",
        "        # stop iterating when the episode finished\n",
        "        if done or t==(max_timesteps-1):\n",
        "            break\n",
        "\n",
        "    # append the episode reward to the reward list\n",
        "    reward_list.append(ep_reward)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # do NOT change this logging code - it is used for automated marking!\n",
        "    log_f.write('episode: {}, reward: {}\\n'.format(episode, ep_reward))\n",
        "    log_f.flush()\n",
        "\n",
        "    # log data to wandb\n",
        "    log_to_wandb(episode,np.array(reward_list).mean(),np.array(reward_list).std(),ep_reward)\n",
        "\n",
        "    ep_reward = 0\n",
        "\n",
        "    # print reward data every so often - add a graph like this in your report\n",
        "    if episode % plot_interval == 0:\n",
        "        plot_data.append([episode, np.array(reward_list).mean(), np.array(reward_list).std()])\n",
        "        reward_list = []\n",
        "        # plt.rcParams['figure.dpi'] = 100\n",
        "        plt.plot([x[0] for x in plot_data], [x[1] for x in plot_data], '-', color='tab:grey')\n",
        "        plt.fill_between([x[0] for x in plot_data], [x[1]-x[2] for x in plot_data], [x[1]+x[2] for x in plot_data], alpha=0.2, color='tab:grey')\n",
        "        plt.xlabel('Episode number')\n",
        "        plt.ylabel('Episode reward')\n",
        "        plt.show()\n",
        "        disp.clear_output(wait=True)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sz_AyXX3OrS5"
      },
      "outputs": [],
      "source": [
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGb5yZY-UFRT"
      },
      "source": [
        "Hyperparameter Search Code\n",
        "- optuna uses Bayesian optimisation\n",
        "\n",
        "(this was not ran in a notebook)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MN6ZIccUUShg"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "\n",
        "\n",
        "def objective(trial):\n",
        "  explore_time = 500 # time where we don't train, fill replay buffer\n",
        "  tr_per_step = 5 # actor's updates per frame/step\n",
        "  tr_pre_step = 64 # actor's updates before a frame/step\n",
        "  hidden_dim = 384 # dimensions of network to use\n",
        "  fade_factor = 7 # fading memory factor\n",
        "  alpha = 0.02 # basicially a stall penalty\n",
        "  exploration_scale_factor=0.2 # exploration scale factor\n",
        "\n",
        "  hyperparams = {\n",
        "      'explore_time': explore_time,\n",
        "      'tr_per_step': tr_per_step,\n",
        "      'hidden_dim': hidden_dim,\n",
        "      'fade_factor': fade_factor,\n",
        "      'alpha':alpha,\n",
        "      'exploration_scale_factor':exploration_scale_factor,\n",
        "      'tr_pre_step':tr_pre_step\n",
        "  }\n",
        "\n",
        "  explore_time = trial.suggest_int('explore_time', 300, 1000)\n",
        "  tr_per_step = trial.suggest_int('tr_per_step', 1, 8)\n",
        "  tr_pre_step = trial.suggest_int('tr_pre_step', 32, 128)\n",
        "  hidden_dim = trial.suggest_int('hidden_dim', 256,512)\n",
        "  fade_factor = trial.suggest_categorical('fade_factor', [1, 10,7,100])\n",
        "  alpha = trial.suggest_float('alpha', 0.01, 0.1)\n",
        "  exploration_scale_factor = trial.suggest_float('exploration_scale_factor', 0.05, 0.5)\n",
        "\n",
        "\n",
        "\n",
        "  # needed variables\n",
        "  Q_learning = False\n",
        "  max_action = 1.0\n",
        "  max_action = max_action*torch.FloatTensor(env.action_space.high).to(device) if env.action_space.is_bounded() else max_action*1.0\n",
        "  total_steps = 0\n",
        "\n",
        "\n",
        "  # is used to init weights of the actor - this is still seeded / deterministic!\n",
        "  def init_weights(m):\n",
        "      if isinstance(m, nn.Linear): torch.nn.init.xavier_uniform_(m.weight)\n",
        "\n",
        "  # search variable\n",
        "  total_reward = 0\n",
        "\n",
        "\n",
        "\n",
        "  # initialise agent\n",
        "  agent = Agent(obs_dim, act_dim, hidden_dim, device, max_action, fade_factor, alpha,exploration_scale_factor)\n",
        "  max_episodes = 150\n",
        "  max_timesteps = 2000\n",
        "\n",
        "\n",
        "  # training procedure:\n",
        "  for episode in range(1, max_episodes+1):\n",
        "\n",
        "\n",
        "\n",
        "      state = env.reset()\n",
        "\n",
        "\n",
        "      # If the agent is exploring, sample some random weights for it - these are seeded!\n",
        "      if not Q_learning and total_steps < explore_time:\n",
        "        agent.actor.apply(init_weights)\n",
        "\n",
        "\n",
        "\n",
        "      if Q_learning: time.sleep(0.5)\n",
        "\n",
        "\n",
        "      for t in range(max_timesteps):\n",
        "          total_steps += 1\n",
        "\n",
        "\n",
        "          if t>=explore_time and not Q_learning:\n",
        "              Q_learning = True\n",
        "              agent.train(tr_pre_step)\n",
        "\n",
        "          action = agent.sample_action(state)\n",
        "\n",
        "\n",
        "\n",
        "          next_state, reward, done, info = env.step(action)\n",
        "\n",
        "\n",
        "\n",
        "          agent.replay_buffer.add(state, action, reward, next_state, done)\n",
        "\n",
        "\n",
        "          if Q_learning:\n",
        "            agent.train(tr_per_step)\n",
        "\n",
        "          state = next_state\n",
        "          total_reward += reward\n",
        "\n",
        "\n",
        "          if done or (not Q_learning and t>=2000): break\n",
        "\n",
        "          # stop iterating when the episode finished\n",
        "          if done or t==(max_timesteps-1):\n",
        "              break\n",
        "\n",
        "\n",
        "  return total_reward\n",
        "\n",
        "\n",
        "def log_trial_info(study, trial):\n",
        "    # Open the log file in append mode, creates it if it doesn't exist\n",
        "    with open(\"trial_log.txt\", \"a\") as log_file:\n",
        "        log_file.write(f\"Trial {trial.number} finished with total reward: {trial.value}, \"\n",
        "                       f\"parameters: {trial.params}\")\n",
        "\n",
        "# Create a study object\n",
        "study = optuna.create_study(direction='maximize')  # Assuming you want to maximize total_reward\n",
        "study.optimize(objective, n_trials=100, callbacks=[log_trial_info])\n",
        "\n",
        "\n",
        "# After all trials, get the best trial information\n",
        "best_trial = study.best_trial\n",
        "print(f\"Best trial total reward: {best_trial.value}\")\n",
        "print(f\"Best trial parameters: {best_trial.params}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5lvKdpeU7WV"
      },
      "source": [
        "**Reinforcement learning agent (TD3)**\n",
        "\n",
        "In our paper we compare our modified implementation of simphony with TD3 - below is the code we used to train a TD3 agent.\n",
        "\n",
        "We used a lot of code from [here](https://github.com/lukau2357/bipedal-walker-td3), this is just a vanilla implementation of TD3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHb5ZmTPU8TQ"
      },
      "source": [
        "Network\n",
        "(same for actor and critic - just uses different dimensions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZQyvqKfU9yU"
      },
      "outputs": [],
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(\n",
        "        self, shape, output_activation, learning_rate,device):\n",
        "        super().__init__()\n",
        "        # initialize the network\n",
        "        layers = []\n",
        "        for i in range(1, len(shape)):\n",
        "            dim1 = shape[i - 1]\n",
        "            dim2 = shape[i]\n",
        "            layers.append(nn.Linear(dim1, dim2))\n",
        "            if i < len(shape) - 1:\n",
        "                layers.append(nn.ReLU())\n",
        "        layers.append(output_activation())\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "        self.device = device\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, state):\n",
        "        return self.network(state)\n",
        "\n",
        "    def gradient_descent_step(self, loss, retain_graph=False):\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward(retain_graph=retain_graph)\n",
        "        self.optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5_8-Va3V0AU"
      },
      "source": [
        "Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsLIFhOgV1mZ"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer():\n",
        "    def __init__(self, obs_dim, act_dim, size=1_000_000):\n",
        "        # use a fixed-size buffer to prevent constant list instantiations\n",
        "        self.states = np.zeros((size, obs_dim))\n",
        "        self.actions = np.zeros((size, act_dim))\n",
        "        self.rewards = np.zeros(size)\n",
        "        self.next_states = np.zeros((size, obs_dim))\n",
        "        self.done_flags = np.zeros(size)\n",
        "        # use a pointer to keep track of where in the buffer we are\n",
        "        self.pointer = 0\n",
        "        # use current size to ensure we don't train on any non-existent data points\n",
        "        self.current_size = 0\n",
        "        self.size = size\n",
        "\n",
        "    def store(\n",
        "        self, state, action, reward, next_state,done_flag):\n",
        "        # store all the data for this transition\n",
        "        ptr = self.pointer\n",
        "        self.states[ptr] = state\n",
        "        self.actions[ptr] = action\n",
        "        self.rewards[ptr] = reward\n",
        "        self.next_states[ptr] = next_state\n",
        "        self.done_flags[ptr] = done_flag\n",
        "        # update the pointer and done_flag size\n",
        "        self.pointer = (self.pointer + 1) % self.size\n",
        "        self.current_size = min(self.current_size + 1, self.size)\n",
        "\n",
        "    def get_mini_batch(self, size):\n",
        "        # ensure size is not bigger than the current size of the buffer\n",
        "        size = min(size, self.current_size)\n",
        "        # generate random indices\n",
        "        indices = np.random.choice(self.current_size, size, replace=False)\n",
        "        # return the mini-batch of transitions\n",
        "        return {\n",
        "            \"states\": self.states[indices],\n",
        "            \"actions\": self.actions[indices],\n",
        "            \"rewards\": self.rewards[indices],\n",
        "            \"next_states\": self.next_states[indices],\n",
        "            \"done_flags\": self.done_flags[indices],\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIJckB97V35S"
      },
      "source": [
        "Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzgQ8IdXV5VN"
      },
      "outputs": [],
      "source": [
        "\n",
        "from copy import deepcopy\n",
        "\n",
        "class Agent():\n",
        "    def __init__(\n",
        "        self, obs_dim,act_dim, learning_rate, gamma, tau, cycle_num=-1):\n",
        "        self.obs_dim = obs_dim\n",
        "        self.act_dim = act_dim\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.cycle_num = cycle_num\n",
        "\n",
        "\n",
        "\n",
        "        self.device = torch.device('cpu')\n",
        "        self.buffer = ReplayBuffer(self.obs_dim, self.act_dim)\n",
        "\n",
        "\n",
        "        # initialize the actor and critics\n",
        "        self.actor = Network([self.obs_dim, 400, 300, self.act_dim],nn.Tanh,learning_rate,self.device)\n",
        "\n",
        "        self.critic1 = Network([self.obs_dim + self.act_dim, 400, 300, 1],nn.Identity,learning_rate,self.device)\n",
        "\n",
        "        self.critic2 = Network([self.obs_dim + self.act_dim, 400, 300, 1],nn.Identity,learning_rate,self.device)\n",
        "\n",
        "        # create target networks\n",
        "        self.target_actor = deepcopy(self.actor)\n",
        "\n",
        "        self.target_critic1 = deepcopy(self.critic1)\n",
        "\n",
        "        self.target_critic2 = deepcopy(self.critic2)\n",
        "\n",
        "    def sample_action(self, state, sigma):\n",
        "\n",
        "        deterministic_action = self.get_deterministic_action(state)\n",
        "        noise = np.random.normal(0, sigma, deterministic_action.shape)\n",
        "        return np.clip(deterministic_action + noise, -1, +1)\n",
        "\n",
        "    def get_deterministic_action(self, state):\n",
        "        actions: torch.Tensor = self.actor.forward(torch.tensor(state, device=self.device))\n",
        "        return actions.cpu().detach().numpy()\n",
        "\n",
        "    def update(self, mini_batch_size, training_sigma, training_clip,update_policy):\n",
        "        # randomly sample a mini-batch from the replay buffer\n",
        "        mini_batch = self.buffer.get_mini_batch(mini_batch_size)\n",
        "        # create tensors to start generating computational graph\n",
        "        states = torch.tensor(mini_batch[\"states\"], requires_grad=True, device=self.device)\n",
        "        actions = torch.tensor(mini_batch[\"actions\"], requires_grad=True, device=self.device)\n",
        "        rewards = torch.tensor(mini_batch[\"rewards\"], requires_grad=True, device=self.device)\n",
        "        next_states = torch.tensor(\n",
        "            mini_batch[\"next_states\"], requires_grad=True, device=self.device\n",
        "        )\n",
        "        dones = torch.tensor(mini_batch[\"done_flags\"], requires_grad=True, device=self.device)\n",
        "        # compute the targets\n",
        "        targets = self.compute_targets(\n",
        "            rewards, next_states, dones, training_sigma, training_clip\n",
        "        )\n",
        "        # do a single step on each critic network\n",
        "        Q1Loss = self.compute_qloss(self.critic1, states, actions, targets)\n",
        "        self.critic1.gradient_descent_step(Q1Loss, True)\n",
        "        Q2Loss = self.compute_qloss(self.critic2, states, actions, targets)\n",
        "        self.critic2.gradient_descent_step(Q2Loss)\n",
        "        if update_policy:\n",
        "            # do a single step on the actor network\n",
        "            policy_loss = self.compute_policy_loss(states)\n",
        "            self.actor.gradient_descent_step(policy_loss)\n",
        "            # update target networks\n",
        "            self.update_target_network(self.target_actor, self.actor)\n",
        "            self.update_target_network(self.target_critic1, self.critic1)\n",
        "            self.update_target_network(self.target_critic2, self.critic2)\n",
        "\n",
        "    def compute_targets(self, rewards, next_states, dones,training_sigma, training_clip):\n",
        "        target_actions = self.target_actor.forward(next_states.float())\n",
        "        # create additive noise for target actions\n",
        "        noise = np.random.normal(0, training_sigma, target_actions.shape)\n",
        "        clipped_noise = torch.tensor(\n",
        "            np.clip(noise, -training_clip, +training_clip), device=self.device\n",
        "        )\n",
        "        target_actions = torch.clip(target_actions + clipped_noise, -1, +1)\n",
        "        # compute targets\n",
        "        target_Q1_values = torch.squeeze(\n",
        "            self.target_critic1.forward(torch.hstack([next_states, target_actions]).float())\n",
        "        )\n",
        "        target_Q2_values = torch.squeeze(\n",
        "            self.target_critic2.forward(torch.hstack([next_states, target_actions]).float())\n",
        "        )\n",
        "        target_Q_values = torch.minimum(target_Q1_values, target_Q2_values)\n",
        "        return rewards + self.gamma*(1 - dones)*target_Q_values\n",
        "\n",
        "    def compute_qloss(\n",
        "        self, network, states, actions, targets):\n",
        "        # compute the MSE of the Q function with respect to the targets\n",
        "        Q_values = torch.squeeze(network.forward(torch.hstack([states, actions]).float()))\n",
        "        return torch.square(Q_values - targets).mean()\n",
        "\n",
        "    def compute_policy_loss(self, states):\n",
        "        actions = self.actor.forward(states.float())\n",
        "        Q_values = torch.squeeze(self.critic1.forward(torch.hstack([states, actions]).float()))\n",
        "        return -Q_values.mean()\n",
        "\n",
        "    def update_target_network(self, target_network, network):\n",
        "        with torch.no_grad():\n",
        "            for target_parameter, parameter in zip(\n",
        "                target_network.parameters(), network.parameters()\n",
        "            ):\n",
        "                target_parameter.mul_(1 - self.tau)\n",
        "                target_parameter.add_(self.tau*parameter)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lVSaRVXV_4q"
      },
      "source": [
        "Hyperparams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tVTjoyMWBBX"
      },
      "outputs": [],
      "source": [
        "# hyperparamaters - original ones\n",
        "gamma = 0.99         # discount factor for rewards\n",
        "learningRate = 3e-4  # learning rate for actor and critic networks\n",
        "tau = 0.005          # tracking parameter used to update target networks slowly\n",
        "actionSigma = 0.1    # contributes noise to deterministic policy output\n",
        "trainingSigma = 0.2  # contributes noise to target actions\n",
        "trainingClip = 0.5   # clips target actions to keep them close to true actions\n",
        "miniBatchSize = 100  # how large a mini-batch should be when updating\n",
        "policyDelay = 2      # how many steps to wait before updating the policy\n",
        "\n",
        "\n",
        "# hyperparamters we found through search\n",
        "gamma= 0.9631181910550325\n",
        "learning_rate= 0.00048294207603125734\n",
        "tau = 0.0026831631289759876\n",
        "action_sigma=0.10499796824145134\n",
        "training_sigma=0.2723359263110223\n",
        "training_clip = 0.49988955249436184\n",
        "mini_batch_size=107\n",
        "policy_delay=3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70PRPXe3WCWT"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Setup logging / Choose enviroment\n",
        "%load_ext wandb\n",
        "ENV_NAME = \"BipedalWalker-v3\" #change to \"BipedalWalkerHardcore-v3\" for the hardcore enviroment\n",
        "wandb.init(project=ENV_NAME,save_code=True)\n",
        "\n",
        "env = gym.make(ENV_NAME)\n",
        "# env = gym.make(\"BipedalWalkerHardcore-v3\") # only attempt this when your agent has solved BipedalWalker-v3\n",
        "env = gym.wrappers.Monitor(env, \"./video\", video_callable=lambda ep_id: ep_id%video_every == 0, force=True)\n",
        "\n",
        "obs_dim = env.observation_space.shape[0]\n",
        "act_dim = env.action_space.shape[0]\n",
        "\n",
        "\n",
        "plot_interval = 10 # update the plot every N episodes\n",
        "video_every = 100 # videos can take a very long time to render so only do it every N episodes\n",
        "\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# Set seeds nice and early - for consistent results\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.use_deterministic_algorithms(True)\n",
        "torch.cuda.manual_seed_all(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGJ-GVYFWFS0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# logging variables\n",
        "ep_reward = 0\n",
        "reward_list = []\n",
        "plot_data = []\n",
        "\n",
        "# we dont submit the this - can only submit one agent log\n",
        "# log_f = open(\"agent-log.txt\",\"w+\")\n",
        "\n",
        "# initialise agent\n",
        "agent = Agent(obs_dim,act_dim,learningRate, gamma, tau)\n",
        "max_episodes = 1000\n",
        "max_timesteps = 2000\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# training procedure:\n",
        "for episode in range(1, max_episodes+1):\n",
        "    state = env.reset()\n",
        "    for t in range(max_timesteps):\n",
        "\n",
        "        action = agent.sample_action(state, action_sigma)\n",
        "\n",
        "\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        agent.buffer.store(state, action, reward, next_state, done)\n",
        "\n",
        "        state = next_state\n",
        "        ep_reward += reward\n",
        "\n",
        "\n",
        "        should_update_policy = t % policy_delay == 0\n",
        "\n",
        "        agent.update(mini_batch_size, training_sigma, training_clip, should_update_policy)\n",
        "\n",
        "\n",
        "\n",
        "        # stop iterating when the episode finished\n",
        "        if done or t==(max_timesteps-1):\n",
        "            break\n",
        "\n",
        "    # append the episode reward to the reward list\n",
        "    reward_list.append(ep_reward)\n",
        "\n",
        "    # do NOT change this logging code - it is used for automated marking! - we dont use this file for submission!\n",
        "    # log_f.write('episode: {}, reward: {}\\n'.format(episode, ep_reward))\n",
        "    # log_f.flush()\n",
        "\n",
        "\n",
        "    # log data to wandb\n",
        "    log_to_wandb(episode,np.array(reward_list).mean(),np.array(reward_list).std(),ep_reward)\n",
        "\n",
        "    ep_reward = 0\n",
        "\n",
        "    # print reward data every so often - add a graph like this in your report\n",
        "    if episode % plot_interval == 0:\n",
        "        plot_data.append([episode, np.array(reward_list).mean(), np.array(reward_list).std()])\n",
        "        reward_list = []\n",
        "        # plt.rcParams['figure.dpi'] = 100\n",
        "        plt.plot([x[0] for x in plot_data], [x[1] for x in plot_data], '-', color='tab:grey')\n",
        "        plt.fill_between([x[0] for x in plot_data], [x[1]-x[2] for x in plot_data], [x[1]+x[2] for x in plot_data], alpha=0.2, color='tab:grey')\n",
        "        plt.xlabel('Episode number')\n",
        "        plt.ylabel('Episode reward')\n",
        "        plt.show()\n",
        "        disp.clear_output(wait=True)\n",
        "\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVJG8cqPWRBr"
      },
      "source": [
        "Hyperparam search\n",
        "\n",
        "This also wasn't ran in a notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKU13Q82Y44t"
      },
      "outputs": [],
      "source": [
        "def objective(trial, env):\n",
        "    score = 0\n",
        "\n",
        "    # HYPERPARAMETERS\n",
        "    gamma = 0.99         # discount factor for rewards\n",
        "    learningRate = 3e-4  # learning rate for actor and critic networks\n",
        "    tau = 0.005          # tracking parameter used to update target networks slowly\n",
        "    actionSigma = 0.1    # contributes noise to deterministic policy output\n",
        "    trainingSigma = 0.2  # contributes noise to target actions\n",
        "    trainingClip = 0.5   # clips target actions to keep them close to true actions\n",
        "    miniBatchSize = 100  # how large a mini-batch should be when updating\n",
        "    policyDelay = 2      # how many steps to wait before updating the policy\n",
        "\n",
        "\n",
        "    gamma = trial.suggest_float('gamma', 0.9, 0.999, log=True)  # discount factor for rewards\n",
        "    learningRate = trial.suggest_float('learningRate', 1e-5, 1e-3, log=True)  # learning rate for actor and critic networks\n",
        "    tau = trial.suggest_float('tau', 0.001, 0.01, log=True)  # tracking parameter used to update target networks slowly\n",
        "    actionSigma = trial.suggest_float('actionSigma', 0.05, 0.2)  # contributes noise to deterministic policy output\n",
        "    trainingSigma = trial.suggest_float('trainingSigma', 0.1, 0.3)  # contributes noise to target actions\n",
        "    trainingClip = trial.suggest_float('trainingClip', 0.1, 1.0)  # clips target actions to keep them close to true actions\n",
        "    miniBatchSize = trial.suggest_int('miniBatchSize', 32, 120)  # how large a mini-batch should be when updating\n",
        "    policyDelay = trial.suggest_int('policyDelay', 1, 5)  # how many steps to wait before updating the policy\n",
        "\n",
        "\n",
        "    \"\"\"**Prepare the environment and wrap it to capture videos**\"\"\"\n",
        "\n",
        "    # Commented out IPython magic to ensure Python compatibility.\n",
        "    # %%capture\n",
        "\n",
        "    # # env = gym.make(\"BipedalWalkerHardcore-v3\") # only attempt this when your agent has solved BipedalWalker-v3\n",
        "    # env = gym.wrappers.Monitor(env, \"./video\", video_callable=lambda ep_id: ep_id%video_every == 0, force=True)\n",
        "    #\n",
        "    obs_dim = env.observation_space.shape[0]\n",
        "    act_dim = env.action_space.shape[0]\n",
        "\n",
        "    print('The environment has {} observations and the agent can take {} actions'.format(obs_dim, act_dim))\n",
        "    print('The device is: {}'.format(device))\n",
        "\n",
        "    if device.type != 'cpu': print('It\\'s recommended to train on the cpu for this')\n",
        "\n",
        "    # in the submission please use seed 42 for verification\n",
        "    seed = 42\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    env.seed(seed)\n",
        "    env.action_space.seed(seed)\n",
        "\n",
        "    # logging variables\n",
        "    ep_reward = 0\n",
        "    reward_list = []\n",
        "    plot_data = []\n",
        "\n",
        "    # initialise agent\n",
        "    agent = Agent(\"plz\",obs_dim,act_dim,learningRate, gamma, tau)\n",
        "    max_episodes = 300\n",
        "    max_timesteps = 2000\n",
        "\n",
        "    # training procedure:\n",
        "    for episode in range(1, max_episodes+1):\n",
        "        print(\"on episode: \", episode)\n",
        "        state = env.reset()\n",
        "        for t in range(max_timesteps):\n",
        "\n",
        "            # select the agent action\n",
        "\n",
        "            action = agent.sample_action(state, actionSigma)\n",
        "\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "\n",
        "            agent.buffer.store(state, action, reward, next_state, done)\n",
        "\n",
        "            state = next_state\n",
        "            ep_reward += reward\n",
        "\n",
        "            shouldUpdatePolicy = t % policyDelay == 0\n",
        "\n",
        "            agent.update(miniBatchSize, trainingSigma, trainingClip, shouldUpdatePolicy)\n",
        "\n",
        "            # stop iterating when the episode finished\n",
        "            if done or t==(max_timesteps-1):\n",
        "                break\n",
        "\n",
        "        # append the episode reward to the reward list\n",
        "        score += ep_reward\n",
        "        reward_list.append(ep_reward)\n",
        "        ep_reward = 0\n",
        "\n",
        "        # print reward data every so often - add a graph like this in your report\n",
        "        if episode % plot_interval == 0:\n",
        "            plot_data.append([episode, np.array(reward_list).mean(), np.array(reward_list).std()])\n",
        "            # plt.rcParams['figure.dpi'] = 100\n",
        "            log_to_wandb(plot_data)\n",
        "    return score\n",
        "\n",
        "\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(lambda trial: objective(trial, env), n_trials=200)\n",
        "wandb.finish()\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
